---
title: "Final Project Modern Statistical Computing"
author: "Martin Blasi and Jonas Wallstein"
date: "2023-03-15"
output: pdf_document
---

## Setup
```{r}
mat <- read.csv2('../data/student-mat.csv')
por <- read.csv2('../data/student-por.csv')

```

```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(coefplot)
library(modelr)
library(openintro)
library(brglm)
library(mombf)
library(pROC)
library(keras)
library(mlbench)
library(mgcv)
library(ggpubr)
library(huxtable)
library(jtools)
```

```{r}
source('routines.R')
```


## 1. Explaining
### 1.1 Linear Regression 
#### Full linear model
```{r}
fitall <- lm(G3~.,data=mat)
summary(fitall)
# bestBIC(G3~., data=mat)
# fit1 <- lm(G3~age +famrel+ absences+ G1+ G2,data=mat)
# summary(fit1)

```

#### Full linear model without grades
```{r}
dfnog <- select(mat, -c("G1", "G2"))
fitallnog <- lm(G3~.,data=dfnog)
summary(fitallnog)
#bestBIC(G3~., data=dfnog)
```

#### Test table
```{r}
export_summs(fitall, fitallnog, digits=4, error_format = "({p.value})", model.names = c("Full Linear model", "Full linear model without grades"), results = 'asis')
```


### 1.2 Poisson Regression
#### Full Poisson
```{r}
fitallp <- glm(G3~.,data=mat, family=poisson())
summary(fitallp)
#bestBIC(G3~.,data=mat, family="poisson")
```
#### Full Poisson without grades
```{r}
fitallpnog <- glm(G3~.,data=dfnog,family="poisson")
summary(fitallpnog)
```
#### Quasipoisson - adjusting for overdispersion
```{r}
fitallqp <- glm(G3~.,data=mat,family="quasipoisson")
summary(fitallqp)
```
#### Quasipoisson without grades
```{r}
fitallqpnog <- glm(G3~.,data=dfnog,family="quasipoisson")
summary(fitallqpnog)
```

### 1.3 Binomial Regression
#### Pass/Fail Model
```{r}
df <- mat
df$pass <- ifelse(df$G3>9, 1 ,0)
dfbin <- select(df, -c("G3"))
fitallb <- glm(pass~.,data=dfbin,family=binomial())
summary(fitallb)
#bestBIC(pass~.  ,data=dfbin, family="binomial")
#fitbin <- glm(pass~Fedu+ famrel+ goout+ Walc+ G2,data=dfbin,family="binomial")
#summary(fitbin)
```

#### Optional: Gradelevels
```{r}
# df$gradelevel <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c("Fail", "Sufficient", "Satisfactory", "Good", "Excellent"))
# df$gradecat <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c(0,1,2,3,4))
# dfless2 <- select(dfless, -c("gradelevel"))
# fitcatall <- glm(gradecat~.,data=dfless2,family=poisson())
# summary(fitcatall)
```

### 1.4 Sub-sample analysis of passing/failing students
#### Sub-sample analysis of students who pass
```{r}
dfpass <- subset(df,pass==1)
#dfpass <- select(dfpass, -c("pass", "resl", "resbn", "predl"))
dffail <- subset(df, pass==0)
#dffail <- select(dffail, -c("pass", "resl", "resbn", "predl"))
fitallpass <- lm(G3~.,data=dfpass)
summary(fitallpass)
#bestBIC(G3~., data=dfpass)
```
--> age, absences and G1 out
-->rather consider significant effects in full model than bestBIC?
-->R^2 over 90%

#### Sub-sample analysis of students who fail
```{r}
fitallfail <- lm(G3~.,data=dffail)
summary(fitallfail)
#bestBIC(G3~.,data=dffail)
```
-->also absences and previous performance
-->R^2 much lower though (close to 60%)

### 1.5 Grade difference
What students have improved their grades over the course of the year? What role did support from the family/school play?

#### Creating dataframe
```{r}
df.diff <- df %>% 
  mutate(gradediff13 = G3 - G1) %>% 
  select(-c("G1", "G2", "G3", "pass")) %>% 
  mutate(improvement = ifelse(gradediff13 >= 0, 1, 0))
```

#### Linear regression on grade difference
```{r}
fitdiff1 <- lm(gradediff13 ~ ., data = df.diff)
summary(fitdiff1)
bestBIC(gradediff13 ~. , data = df.diff)
```

#### Binomial regression: improvement yes/no
```{r}
fitdiff2 <- glm(improvement ~ . -gradediff13, data = df.diff, family = "binomial")
summary(fitdiff2)
bestBIC(improvement ~. -gradediff13, data = df.diff)

```
- failures,  romantic relationships and absences seem to be important factors.
- Interestingly, no type of support has a significant effect, are there heterogeneous effects and reverse causality? Can we test that somehow?
- With binary outcome improvement yes/no absences and resonother seem important, although bestBIC suggests only age as predictor
- If improvement is relaxed to >= 0 instead of >0, paidyes becomes significant and positive -> interesting!

#### Plots
```{r}
table(df.diff$improvement)

library(ggpubr)
ggplot(data = df.diff, aes(x = gradediff13)) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_overlay_normal_density(linetype = "dashed")
```

### 1.6 Subsample analysis of students that improved vs. those that did not
```{r}
df.posdiff <- subset(df.diff, improvement == 1)
fitdiff3 <- glm(gradediff13 ~ . -improvement, data = df.posdiff)
summary(fitdiff3)
#bestBIC(gradediff13 ~. - improvement, data = df.posdiff)

df.negdiff <- subset(df.diff, improvement == 0)
fitdiff4 <- glm(gradediff13 ~ . -improvement, data = df.negdiff)
summary(fitdiff4)
#bestBIC(gradediff13 ~. - improvement, data = df.negdiff)
#fitdiff5 <- glm(gradediff13 ~ school + age + traveltime + failures + romantic + Walc + absences, data = df.negdiff)
#summary(fitdiff5)
```
-> mean-center it?
For improvers

## 2. Prediction model
-->for full linear model and the binary case

### 2.1 Training-Test Split
```{r}
## 75% of the sample size
smp_size <- floor(0.90 * nrow(dfbin))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(dfbin)), size = smp_size)

train <- dfbin[train_ind, ]
test <- dfbin[-train_ind, ]
```

#### Rerun the models on the training data
```{r}
fitbin2= glm(pass~Fedu+ famrel+ goout+ Walc+ G2,data=train,family="binomial")
```

#### Make predictions on test data
```{r}
pibintest= predict(fitbin2, type='response', newdata=test)
table(pibintest > 0.5, test$pass)
```

#### Assess mis-classification in test data
```{r}
cost_misclass= function(yobs, ypred) {
  err1= (ypred > 0.5) & (yobs==0)
  err2= (ypred < 0.5) & (yobs==1)
  ans= sum(err1 | err2) / length(yobs)
  return(ans)
}
misclas= c(cost_misclass(test$pass, pibintest))
names(misclas)= c('model 1')
misclas
```
#### Compare to misclassification in training data 
```{r}
pibin= predict(fitbin2, type='response', data = train) # data??
loss.insample= c(cost_misclass(train$pass, pibin))
names(loss.insample)= c('model 1')
loss.insample
table(pibin > 0.5, train$pass)
```

### 2.2 Cross-validation
```{r}
fitbin3= glm(pass~Fedu+ famrel+ goout+ Walc+ G2,data=dfbin,family="binomial")
fitbin3cv= cv.glm(dfbin, fitbin3, cost=cost_loglik_logistic, K=10)
loss= sqrt(fitbin3cv$delta)
loss

```
- We should compare that to another model!


## APPENDIX

### A.1 Validating Assumptions

#### A.1.1 LM-model check
-->Models: fit1=bestBIC (fitall auch?)
```{r}
df$predl= predict(fitall)
df$resl= residuals(fitall)
```

Linearity
```{r}
ggplot(df, aes(predl, resl)) +
  geom_point() +
  geom_smooth() +
  geom_abline(slope=0, intercept=0, col='gray') +
  labs(x='Model prediction', y='Residuals')
```
Constant residual variance
```{r}
ggplot(df, aes(x=predl, y=resl)) + 
  geom_boxplot(mapping = aes(group = cut_width(predl, 0.2))) +
  labs(x='Model prediction', y='Residuals')
```
Error normality
```{r}
ggplot(df, aes(x=resl)) +
  geom_histogram(aes(y= ..density..)) +
  stat_overlay_normal_density(linetype = "dashed") +
  labs(x='Residuals')
```

```{r}
ggplot(df, aes(sample=scale(resl))) +
  geom_qq() +
  geom_abline(slope=1, intercept=0)
```
-->Errors are not normal and variance is not constant!!
=>Apply robust standard errors

#### A.1.2 Poisson-check
```{r}
poires= mutate(df, pred= predict(fitallp), resdev= residuals(fitallp, type='deviance'), respearson= residuals(fitallp, type='pearson'))
```

```{r}
ggplot(poires, aes(pred, respearson)) + geom_point() + geom_smooth() + labs(x='Predicted', y='Pearson residual')
```

```{r}
poires2= mutate(poires, predcut= cut_number(pred, 10))
ggplot(poires2, aes(x=predcut, y=respearson)) + geom_boxplot()
```

```{r}
mean(poires$respearson)
sd(poires$respearson)
mean(df$G3)
var(df$G3)
```
=>Huge overdispersion and variance not constant, errors not normal

Error normality?
```{r}
# ggplot(poires, aes(x=respearson)) +
#   geom_histogram(aes(y= ..density..)) +
#   stat_overlay_normal_density(linetype = "dashed") +
#   labs(x='Residuals')
# ggplot(poires, aes(sample=scale(respearson))) +
#   geom_qq() +
#   geom_abline(slope=1, intercept=0)
```

#### A.1.3 Binomial check
```{r}
binres= mutate(df, pred= predict(fitallb), resdev= residuals(fitallb, type='deviance'), respearson= residuals(fitallb, type='pearson'))
```

```{r}
ggplot(binres, aes(pred, respearson)) + geom_point() + geom_smooth() + labs(x='Predicted', y='Pearson residual')
```
Residuals seem roughly centered at zero

Constant residual variance
```{r}
ggplot(binres, aes(x=pred, y=respearson)) + 
  geom_boxplot(mapping = aes(group = cut_width(pred, 0.2))) +
  labs(x='Model prediction', y='Residuals')
```

### A.2 Grade difference Plots
```{r}
ggplot(data=df, aes(G1,G2,color=pass))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```
```{r}
ggplot(data=df, aes(G1,G3,color=pass,scale))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```
```{r}
ggplot(data=df, aes(G2,G3,color=pass))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```