---
title: "StuPerf_expl"
author: "Martin Blasi"
date: "2023-03-15"
output: html_document
---
```{r}

mat <- read.csv2('../data/student-mat.csv')
por <- read.csv2('../data/student-por.csv')
```

-------------------------------------------
```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(coefplot)
library(modelr)
library(openintro)
library(brglm)
library(mombf)
library(pROC)
library(keras)
library(mlbench)
library(mgcv)

```

```{r}
source('routines.R')
```


<!-- Dimension reduction - does not work with categorical variables -->
<!-- ```{r} -->
<!-- X <- as.matrix(mat) -->

<!-- Xs <- scale(X) -->
<!-- R <- cov(Xs) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- l <- eigen(R)$values -->
<!-- V <- eigen(R)$vectors -->
<!-- Y <- Xs %*% V -->
<!-- # the portion of the variance that retains each PC -->
<!-- R2 = l / sum(l) *100 -->
<!-- # accumulating this quantities: -->
<!-- cumsum( l / sum(l)) -->

<!-- #automatically -->
<!-- library(ade4) -->
<!-- library(factoextra) -->
<!-- pca.mat = dudi.pca(mat, scannf = FALSE) -->
<!-- pca.mat$eig -->
<!-- cumsum(pca.mat$eig)/sum(pca.mat$eig) -->
<!-- fviz_eig(pca.mat, geom = "bar", bar_width = 0.3, addlabels=T) + ggtitle("") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #Plot -->
<!-- plot(Y[,1:2], -->
<!--   type="n", # don't plot anything now -->
<!--   asp = 1, # aspect ratio, very important! -->
<!--   main=paste("Variables plot, expl. var is ", -->
<!--                round(R2[1]+R2[2],1), " %"), -->
<!--   xlab=paste("CP1(", round(R2[1],1), "%)"), # title x axis -->
<!--   ylab=paste("CP2(", round(R2[2],1), "%)") # title y axis -->
<!-- ) -->

<!-- # add the axis -->
<!-- abline(v=0, col="blue") -->
<!-- abline(h=0, col="blue") -->

<!-- # the dots and names -->
<!-- points(Y[,1:2], pch=".") # plot a dot per row -->
<!-- text(Y[,1:2],row.names(df.empref2),cex=0.6) -->
<!-- ``` -->

<!-- SVD and Variables Plot -->
<!-- ```{r} -->
<!-- #Carry out the Singular Value Decomposition -->
<!-- svd <- svd(Xs) -->


<!-- Utilde <- svd$u[,1:2] %*% sqrt(diag(svd$d[1:2])) -->
<!-- Vtilde <- svd$v[,1:2] %*% sqrt(diag(svd$d[1:2])) -->


<!-- nn <- paste(rownames(Xs)) -->
<!-- xlim <- ylim <- range(c(range(1.5*Vtilde))) -->
<!-- plot(NULL,xlab='First biplot component',ylab='Second biplot component',xlim=xlim,ylim=ylim,cex.lab=1.25) -->
<!-- arrows(x0=0,y0=0,x1=Vtilde[,1],y1=Vtilde[,2],col='red') -->
<!-- text(.9*Vtilde[,1],Vtilde[,2],paste(colnames(Xs)),pos=c(1,3),col='red',cex=1.25) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- nn <- paste(rownames(Xs)) -->
<!-- xlim <- ylim <- range(c(range(Vtilde),range(1.5*Vtilde))) -->
<!-- plot(Utilde,xlab='First biplot component',ylab='Second biplot component',xlim=xlim,ylim=ylim,cex.lab=1.25) -->
<!-- text(Utilde[,1:2],nn,pos=c(4,1,2),cex=1.25) -->
<!-- arrows(x0=0,y0=0,x1=Vtilde[,1],y1=Vtilde[,2],col='red') -->
<!-- text(.9*Vtilde[,1],Vtilde[,2],paste(colnames(Xs)),pos=c(1,3),col='red',cex=1.25) -->

<!-- ``` -->

```{r}
fitall <- lm(G3~.,data=mat)
summary(fitall)
```

```{r}
bestBIC(G3~., data=mat)
```

```{r}
fit1 <- lm(G3~age +famrel+ absences+ G1+ G2,data=mat)
summary(fit1)
```

```{r}
fitallp <- glm(G3~.,data=mat, family=poisson())
summary(fitallp)
```

## What students have improved their grades over the course of the year? What role did support from the family/school play?

```{r}
df.diff <- df %>% 
  mutate(gradediff13 = G3 - G1) %>% 
  select(-c("G1", "G2", "G3", "pass", "gradecat", "gradelevel")) %>% 
  mutate(improvement = ifelse(gradediff13 >= 0, 1, 0))

fitdiff1 <- lm(gradediff13 ~ ., data = df.diff)
summary(fitdiff1)
bestBIC(gradediff13 ~. , data = df.diff)

fitdiff2 <- glm(improvement ~ . -gradediff13, data = df.diff, family = "binomial")
summary(fitdiff2)
bestBIC(improvement ~. -gradediff13, data = df.diff)

```
- failures,  romantic relationships and absences seem to be important factors.
- Interestingly, no type of support has a significant effect, are there heterogeneous effects and reverse causality? Can we test that somehow?
- With binary outcome improvement yes/no absences and resonother seem important, although bestBIC suggests only age as predictor
- If improvement is relaxed to >= 0 instead of >0, paidyes becomes significant and positive -> interesting!
```{r}
table(df.diff$improvement)

library(ggpubr)
ggplot(data = df.diff, aes(x = gradediff13)) + 
  geom_histogram(aes(y = ..density..)) + 
  stat_overlay_normal_density(linetype = "dashed")
```



```{r}
df <- mat
df$pass <- ifelse(df$G3>9, 1 ,0)
```

```{r}
dfless <- select(df, -c("G3"))
fitallb <- glm(pass~.,data=dfless,family=binomial())
summary(fitallb)
```

```{r}
bestBIC(pass~.  ,data=dfless, family="binomial")
```

```{r}
fitbin <- glm(pass~Fedu+ famrel+ goout+ Walc+ G2,data=dfless,family="binomial")
summary(fitbin)
```

```{r}
df$gradelevel <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c("Fail", "Sufficient", "Satisfactory", "Good", "Excellent"))
df$gradecat <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c(0,1,2,3,4))
```

```{r}
dfless2 <- select(df, -c("gradelevel"))
fitcatall <- glm(gradecat~.,data=dfless2,family=poisson())
summary(fitcatall)
```

Modell-Annahmen prÃ¼fen:
- Grade regression: linear or poisson - assumptions behind (error normality + constant variance & overdispersion)
- Binary regression: error normality

Prediction model: cross-validation and percent of mis-classification