---
title: "StuPerf_expl"
author: "Martin Blasi"
date: "2023-03-15"
output: html_document
---
```{r}
setwd("C:/MARTIN/UPF/UPF Studies/Second trimester/MSC/final project/1STUDENT/student")

mat <- read.csv2('student-mat.csv')
por <- read.csv2('student-por.csv')
```

```{r, warning=FALSE}
library(tidyverse)
library(boot)
library(coefplot)
library(modelr)
library(openintro)
library(brglm)
library(mombf)
library(pROC)
library(keras)
library(mlbench)
library(mgcv)
```

```{r}
source('~/github/statcomp/code/routines.R')
```

Dimension reduction - does not work with categorical variables
```{r}
X <- as.matrix(mat)

Xs <- scale(X)
R <- cov(Xs)
```

```{r}
l <- eigen(R)$values
V <- eigen(R)$vectors
Y <- Xs %*% V
# the portion of the variance that retains each PC
R2 = l / sum(l) *100
# accumulating this quantities:
cumsum( l / sum(l))

#automatically
library(ade4)
library(factoextra)
pca.mat = dudi.pca(mat, scannf = FALSE)
pca.mat$eig
cumsum(pca.mat$eig)/sum(pca.mat$eig)
fviz_eig(pca.mat, geom = "bar", bar_width = 0.3, addlabels=T) + ggtitle("")
```

```{r}
#Plot
plot(Y[,1:2],
  type="n", # don't plot anything now
  asp = 1, # aspect ratio, very important!
  main=paste("Variables plot, expl. var is ",
               round(R2[1]+R2[2],1), " %"),
  xlab=paste("CP1(", round(R2[1],1), "%)"), # title x axis
  ylab=paste("CP2(", round(R2[2],1), "%)") # title y axis
)

# add the axis
abline(v=0, col="blue")
abline(h=0, col="blue")

# the dots and names
points(Y[,1:2], pch=".") # plot a dot per row
text(Y[,1:2],row.names(df.empref2),cex=0.6)
```

SVD and Variables Plot
```{r}
#Carry out the Singular Value Decomposition
svd <- svd(Xs)


Utilde <- svd$u[,1:2] %*% sqrt(diag(svd$d[1:2]))
Vtilde <- svd$v[,1:2] %*% sqrt(diag(svd$d[1:2]))


nn <- paste(rownames(Xs))
xlim <- ylim <- range(c(range(1.5*Vtilde)))
plot(NULL,xlab='First biplot component',ylab='Second biplot component',xlim=xlim,ylim=ylim,cex.lab=1.25)
arrows(x0=0,y0=0,x1=Vtilde[,1],y1=Vtilde[,2],col='red')
text(.9*Vtilde[,1],Vtilde[,2],paste(colnames(Xs)),pos=c(1,3),col='red',cex=1.25)
```

```{r}
nn <- paste(rownames(Xs))
xlim <- ylim <- range(c(range(Vtilde),range(1.5*Vtilde)))
plot(Utilde,xlab='First biplot component',ylab='Second biplot component',xlim=xlim,ylim=ylim,cex.lab=1.25)
text(Utilde[,1:2],nn,pos=c(4,1,2),cex=1.25)
arrows(x0=0,y0=0,x1=Vtilde[,1],y1=Vtilde[,2],col='red')
text(.9*Vtilde[,1],Vtilde[,2],paste(colnames(Xs)),pos=c(1,3),col='red',cex=1.25)

```

```{r}
fitall <- lm(G3~.,data=mat)
summary(fitall)
```


```{r}
bestBIC(G3~., data=mat)
```

```{r}
fit1 <- lm(G3~age +famrel+ absences+ G1+ G2,data=mat)
summary(fit1)
```

```{r}
dfnog <- select(mat, -c("G1", "G2"))
```

```{r}
fitallnog <- lm(G3~.,data=dfnog)
summary(fitallnog)
```

```{r}
bestBIC(G3~., data=dfnog)
```



```{r}
fitallp <- glm(G3~.,data=mat, family=poisson())
summary(fitallp)
```
```{r}
bestBIC(G3~.,data=mat, family="poisson")
```

```{r}
fitallpnog <- glm(G3~.,data=dfnog,family="poisson")
summary(fitallpnog)
```
```{r}
fitallqp <- glm(G3~.,data=mat,family="quasipoisson")
summary(fitallqp)
```
```{r}
fitallqpnog <- glm(G3~.,data=dfnog,family="quasipoisson")
summary(fitallqpnog)
```




```{r}
df <- mat
df$pass <- ifelse(df$G3>9, 1 ,0)
```

```{r}
dfbin <- select(df, -c("G3"))
fitallb <- glm(pass~.,data=dfbin,family=binomial())
summary(fitallb)
```

```{r}
bestBIC(pass~.  ,data=dfbin, family="binomial")
```

```{r}
fitbin <- glm(pass~Fedu+ famrel+ goout+ Walc+ G2,data=dfless,family="binomial")
summary(fitbin)
```

```{r}
df$gradelevel <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c("Fail", "Sufficient", "Satisfactory", "Good", "Excellent"))
df$gradecat <- cut(df$G3, breaks=c(0,9,11,13,15,20), labels=c(0,1,2,3,4))
```

```{r}
#dfless2 <- select(dfless, -c("gradelevel"))
#fitcatall <- glm(gradecat~.,data=dfless2,family=poisson())
#summary(fitcatall)
```

Plots
```{r}
ggplot(data=df, aes(G1,G2,color=pass))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```
```{r}
ggplot(data=df, aes(G1,G3,color=pass,scale))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```
```{r}
ggplot(data=df, aes(G2,G3,color=pass))+
  geom_point(position="jitter")+
  geom_smooth()+
  geom_abline(slope=1)+
  coord_cartesian(xlim=c(0,20))
```


Next TO DO
a)Modell-Annahmen prüfen:
- Grade regression: linear => error normality + constant variance
or poisson - error normality + constant variance & overdispersion

+ censored count data prüfen!! Wößmann Umgang mit student test outcomes

- Binary regression: error normality
=>in den Appendix

b)Explanation Teil weiter ausführen
+alles für Portugisiesch laufen lassen und vergleichen
+unter den, die bestehen, Arbeitszeit relativ zur Note vergleichen (Hypothese: Mathe=Talentfach, Sprache=Übungsfach)

Clustering? nach noten über die Perioden und ob die gruppen andere charakteristika haben
- G3-G1: new outcome, which factors matter here? e.g. paid classes

c)Prediction Teil bauen
Prediction model: cross-validation and percent of mis-classification

-->random forest ausprobieren? wenn wir Zeit und Lust haben


APPENDIX

Poisson-check
```{r}
poires= mutate(df, pred= predict(fitallp), resdev= residuals(fitallp, type='deviance'), respearson= residuals(fitallp, type='pearson'))
```

```{r}
ggplot(poires, aes(pred, respearson)) + geom_point() + geom_smooth() + labs(x='Predicted', y='Pearson residual')
```

```{r}
poires2= mutate(poires, predcut= cut_number(pred, 10))
ggplot(poires2, aes(x=predcut, y=respearson)) + geom_boxplot()
```

```{r}
mean(poires$respearson)
sd(poires$respearson)
mean(df$G3)
var(df$G3)
```

